import requests
import json
import time
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

class AisiMuScraper:

    def __init__(self, config_path="config.json"):
        self.config = self._load_config(config_path)

        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": self.config.get("user_agent")
        })

        self.category_urls = {}     # {url: åˆ†ç±»å}
        self.results = {}           # {ç›´æ’­url: ä¸»æ’­/æˆ¿é—´å}
        self.old_urls = set()
        self.new_urls = set()

        self.output_dir = "output"
        os.makedirs(self.output_dir, exist_ok=True)

        self._load_history()

    # ================= åŸºç¡€ =================

    def _load_config(self, path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def tg(self, text):
        token = self.config.get("tg_token")
        chat_id = self.config.get("tg_chat_id")
        if not token or not chat_id:
            return
        try:
            requests.post(
                f"https://api.telegram.org/bot{token}/sendMessage",
                data={"chat_id": chat_id, "text": text},
                timeout=10
            )
        except Exception as e:
            print(f"[AISIMU] Telegram æ¶ˆæ¯å‘é€å¤±è´¥: {e}")

    def tg_file(self, filepath, caption=""):
        token = self.config.get("tg_token")
        chat_id = self.config.get("tg_chat_id")
        if not token or not chat_id:
            return
        try:
            with open(filepath, "rb") as f:
                requests.post(
                    f"https://api.telegram.org/bot{token}/sendDocument",
                    data={
                        "chat_id": chat_id,
                        "caption": caption
                    },
                    files={"document": f},
                    timeout=30
                )
        except Exception as e:
            print(f"[AISIMU] Telegram æ–‡ä»¶å‘é€å¤±è´¥: {e}")

    # ================= ç™»å½• =================

    def login(self):
        print("[AISIMU] å°è¯•ç™»å½•:", self.config["login_url"])
        try:
            r = self.session.get(self.config["login_url"], timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")

            payload = {
                self.config["username_field"]: self.config["username"],
                self.config["password_field"]: self.config["password"]
            }

            token_field = self.config.get("csrf_token_field")
            if token_field:
                token = soup.find("input", {"name": token_field})
                if token:
                    payload[token_field] = token.get("value")

            r = self.session.post(
                self.config["login_url"],
                data=payload,
                allow_redirects=True,
                timeout=10
            )

            if self.config["login_failed_check_text"] in r.text:
                print("[AISIMU] âŒ ç™»å½•å¤±è´¥")
                self.tg("âŒ AISIMU ç™»å½•å¤±è´¥")
                return False

            print("[AISIMU] ç™»å½•æˆåŠŸ")
            return True

        except Exception as e:
            print("[AISIMU] ç™»å½•å¼‚å¸¸:", e)
            self.tg("âŒ AISIMU ç™»å½•å¼‚å¸¸")
            return False

    # ================= åˆ†ç±»é¡µ =================

    def fetch_index(self):
        r = self.session.get(self.config["logged_in_expected_url"], timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")

        for a in soup.select('a[href*="zblist.php"]'):
            name = a.text.strip()
            url = urljoin(self.config["logged_in_expected_url"], a["href"])
            self.category_urls[url] = name

        print(f"[AISIMU] å‘ç°åˆ†ç±»é¡µ: {len(self.category_urls)}")

    # ======== æŠ“å– + è¿‡æ»¤è§„åˆ™ï¼ˆæŒ‰ä½ è¦æ±‚ä¿®æ­£ï¼‰========
    def fetch_category(self, url, cname, idx, total):
        try:
            r = self.session.get(url, timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")

            for tr in soup.select("table tr"):
                tds = tr.find_all("td")
                if len(tds) < 4:
                    continue

                room_name = tds[2].get_text(strip=True)
                live = tds[3].get_text(strip=True)

                if not live.startswith("http"):
                    continue

                # === ä½ çš„æ ¸å¿ƒè¿‡æ»¤è§„åˆ™ ===
                banned_words = ["å¹¿æ’­", "æŸ¥çœ‹ä¸»æ’­", "æ”¯ä»˜å®é£æ§è§£é™¤,ä¹‹å£°,å®åŠ›å¸¦é£,è´¢ç»"]
                if any(w in room_name for w in banned_words):
                    continue

                # åªä¿ç•™çœŸæ­£çš„ä¸»æ’­å
                self.results[live] = room_name

            print(f"[AISIMU] åˆ†ç±»é¡µè¿›åº¦: {idx}/{total}")

        except Exception as e:
            print(f"[AISIMU] âœ– åˆ†ç±»å¤±è´¥: {cname} -> {e}")

    # ================= å¢é‡ =================

    def _load_history(self):
        path = os.path.join(self.output_dir, "history.txt")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                self.old_urls = set(x.strip() for x in f if x.strip())

    def _save_history(self):
        path = os.path.join(self.output_dir, "history.txt")
        with open(path, "w", encoding="utf-8") as f:
            for u in sorted(self.results.keys()):
                f.write(u + "\n")

    # ================= å¤šçº¿ç¨‹æ£€æµ‹ =================

    def check_stream(self, url):
        try:
            r = self.session.head(
                url,
                timeout=(2, 4),
                allow_redirects=True
            )
            ok = r.status_code in (200, 301, 302)
            r.close()
            return ok
        except Exception:
            return False

    def validate_streams(self):
        print("[AISIMU] å¤šçº¿ç¨‹æ£€æµ‹ç›´æ’­æºå¯ç”¨æ€§...")

        valid = {}
        total = len(self.results)

        with ThreadPoolExecutor(max_workers=10) as pool:
            future_map = {
                pool.submit(self.check_stream, url): (url, name)
                for url, name in self.results.items()
            }

            for i, future in enumerate(as_completed(future_map), 1):
                url, name = future_map[future]
                try:
                    if future.result():
                        valid[url] = name
                except Exception:
                    pass

                if i % 20 == 0 or i == total:
                    print(f"[AISIMU] æ£€æµ‹è¿›åº¦: {i}/{total}")

        self.results = valid
        print(f"[AISIMU] âœ… æ£€æµ‹å®Œæˆï¼Œå¯ç”¨æº: {len(valid)}/{total}")

    # ================= M3U å¯¼å‡ºï¼ˆæŒ‰ä½ è¦æ±‚ä¿®æ­£ï¼‰ =================

    def export_m3u(self):
        lines = ["#EXTM3U"]

        for url, room_name in self.results.items():
            # ğŸ‘‰ è¿™é‡Œå·²ç»å»æ‰ group-title="æŸ¥çœ‹ä¸»æ’­"
            lines.append(f'#EXTINF:-1,{room_name}')
            lines.append(url)

        path = os.path.join(self.output_dir, "aisimu.m3u")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print("[AISIMU] M3U å¯¼å‡ºå®Œæˆ:", path)
        return path

    # ================= TXT å¯¼å‡º =================

    def export_txt(self):
        path = os.path.join(self.output_dir, "aisimu.txt")

        with open(path, "w", encoding="utf-8") as f:
            for url, room_name in self.results.items():
                f.write(f"{room_name}\t{url}\n")

        print("[AISIMU] TXT å¯¼å‡ºå®Œæˆ:", path)
        return path

    # ================= JSON å¯¼å‡º =================

    def export_json(self):
        path = os.path.join(self.output_dir, "aisimu.json")

        data = [
            {"name": name, "url": url}
            for url, name in self.results.items()
        ]

        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print("[AISIMU] JSON å¯¼å‡ºå®Œæˆ:", path)
        return path

    # ================= ä¸»æµç¨‹ =================

    def run(self):
        if not self.login():
            return

        self.fetch_index()

        total = len(self.category_urls)
        with ThreadPoolExecutor(max_workers=6) as pool:
            tasks = []
            for i, (url, name) in enumerate(self.category_urls.items(), 1):
                tasks.append(pool.submit(self.fetch_category, url, name, i, total))
            for _ in as_completed(tasks):
                pass

        self.validate_streams()

        self.new_urls = set(self.results) - self.old_urls
        if self.new_urls:
            self.tg(f"ğŸ†• æ–°å¢ç›´æ’­æº {len(self.new_urls)} æ¡")

        m3u_path = self.export_m3u()
        txt_path = self.export_txt()
        json_path = self.export_json()

        self._save_history()

        self.tg_file(
            m3u_path,
            caption=f"âœ… AISIMU é‡‡é›†å®Œæˆ\næœ‰æ•ˆæº: {len(self.results)}"
        )

        print("[AISIMU] å…¨æµç¨‹å®Œæˆï¼Œè„šæœ¬é€€å‡º")


if __name__ == "__main__":
    AisiMuScraper().run()
